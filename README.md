ğŸ“¢ Introduction
This repository combines retrieval-based and fine-tuned LLM approaches to enhance AI-powered question-answering. The RAG pipeline utilizes ChromaDB & LangChain to fetch relevant content, while the fine-tuned LLM (LLaMA-2-7B / Mistral-7B) generates responses tailored to structured Q&A datasets.

1ï¸âƒ£ Retrieval-Augmented Generation (RAG) with ChromaDB & LangChain
A pipeline designed for efficient question-answering (QA) on TechCrunch articles using ChromaDB for retrieval and OpenAI GPT-4 / Hugging Face models for response generation.

ğŸ”¹ Key Features
âœ… Semantic search using ChromaDB
âœ… Context-aware QA with LangChain
âœ… Supports multiple documents for broader insights
âœ… Custom LLM integration with Mistral-7B
âœ… Secure API handling via .env

ğŸ”§ Setup & Usage
1ï¸âƒ£ Clone the repo & install dependencies
2ï¸âƒ£ Extract articles and run the Jupyter notebook
3ï¸âƒ£ Query documents using GPT-4 or Hugging Face LLM

ğŸ”— Notebook: VD_ChromaDB_+_Langchain_QA_Multiple_documents.ipynb
ğŸ”— Custom LLM Script: Hugging_Face_LLm.py

2ï¸âƒ£ Fine-Tuning an LLM with QLoRA
A streamlined approach to fine-tune LLaMA-2-7B / Mistral-7B on structured Q&A pairs generated from .txt summaries.

ğŸ”¹ Key Features
âœ… Q&A dataset generation from .txt files
âœ… QLoRA-based fine-tuning (4-bit quantization)
âœ… Optimized training with bitsandbytes, LoRA, transformers
âœ… Saves fine-tuned models for real-time inference

ğŸ”§ Setup & Usage
1ï¸âƒ£ Convert .txt summaries into structured Q&A pairs (generate_qa.py)
2ï¸âƒ£ Fine-tune the LLM using QLoRA (fine_tune.py)
3ï¸âƒ£ Run inference on the fine-tuned model (inference.py)

ğŸ”¹ Choosing the Right Model:

Mistral-7B: Faster inference, efficient for low-resource setups
LLaMA-2-7B: Stronger reasoning and contextual understanding
